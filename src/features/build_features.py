import polars as pl
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score 


def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:
    # implement here all desired dtypes for tables
    # the following is just an example
    for col in df.columns:
        # last letter of column name will help you determine the type
        if col[-1] in ("P", "A"):
            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))

    return df

def convert_strings(df: pd.DataFrame) -> pd.DataFrame:
    for col in df.columns:  
        if df[col].dtype.name in ['object', 'string']:
            df[col] = df[col].astype("string").astype('category')
            current_categories = df[col].cat.categories
            new_categories = current_categories.to_list() + ["Unknown"]
            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)
            df[col] = df[col].astype(new_dtype)
    return df

def from_polars_to_pandas(
        data: pl.DataFrame,
        cols_pred: list,
        case_ids: pl.DataFrame
    ) -> pl.DataFrame:
    return (
        data.filter(pl.col("case_id").is_in(case_ids))[["case_id", "WEEK_NUM", "target"]].to_pandas(),
        data.filter(pl.col("case_id").is_in(case_ids))[cols_pred].to_pandas(),
        data.filter(pl.col("case_id").is_in(case_ids))["target"].to_pandas()
    )

def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):
    gini_in_time = base.loc[:, ["WEEK_NUM", "target", "score"]]\
        .sort_values("WEEK_NUM")\
        .groupby("WEEK_NUM")[["target", "score"]]\
        .apply(lambda x: 2*roc_auc_score(x["target"], x["score"])-1).tolist()
    
    x = np.arange(len(gini_in_time))
    y = gini_in_time
    a, b = np.polyfit(x, y, 1)
    y_hat = a*x + b
    residuals = y - y_hat
    res_std = np.std(residuals)
    avg_gini = np.mean(gini_in_time)
    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std

if __name__ == '__main__':
    dataPath = "D:/Development/kaggle_home_credit/data/raw/home-credit-credit-risk-model-stability/"
    train_basetable = pl.read_csv(dataPath + "csv_files/train/train_base.csv")
    train_static = pl.concat(
        [
            pl.read_csv(dataPath + "csv_files/train/train_static_0_0.csv").pipe(set_table_dtypes),
            pl.read_csv(dataPath + "csv_files/train/train_static_0_1.csv").pipe(set_table_dtypes),
        ],
        how="vertical_relaxed",
    )
    train_static_cb = pl.read_csv(dataPath + "csv_files/train/train_static_cb_0.csv").pipe(set_table_dtypes)
    train_person_1 = pl.read_csv(dataPath + "csv_files/train/train_person_1.csv").pipe(set_table_dtypes) 
    train_credit_bureau_b_2 = pl.read_csv(dataPath + "csv_files/train/train_credit_bureau_b_2.csv").pipe(set_table_dtypes) 

    test_basetable = pl.read_csv(dataPath + "csv_files/test/test_base.csv")
    test_static = pl.concat(
        [
            pl.read_csv(dataPath + "csv_files/test/test_static_0_0.csv").pipe(set_table_dtypes),
            pl.read_csv(dataPath + "csv_files/test/test_static_0_1.csv").pipe(set_table_dtypes),
            pl.read_csv(dataPath + "csv_files/test/test_static_0_2.csv").pipe(set_table_dtypes),
        ],
        how="vertical_relaxed",
    )
    test_static_cb = pl.read_csv(dataPath + "csv_files/test/test_static_cb_0.csv").pipe(set_table_dtypes)
    test_person_1 = pl.read_csv(dataPath + "csv_files/test/test_person_1.csv").pipe(set_table_dtypes) 
    test_credit_bureau_b_2 = pl.read_csv(dataPath + "csv_files/test/test_credit_bureau_b_2.csv").pipe(set_table_dtypes) 

    ## Feature Engineering
    # We need to use aggregation functions in tables with depth > 1, so tables that contain num_group1 column or 
    # also num_group2 column.
    train_person_1_feats_1 = train_person_1.group_by("case_id").agg(
        pl.col("mainoccupationinc_384A").max().alias("mainoccupationinc_384A_max"),
        (pl.col("incometype_1044T") == "SELFEMPLOYED").max().alias("mainoccupationinc_384A_any_selfemployed")
    )

    # Here num_group1=0 has special meaning, it is the person who applied for the loan.
    train_person_1_feats_2 = train_person_1.select(["case_id", "num_group1", "housetype_905L"]).filter(
        pl.col("num_group1") == 0
    ).drop("num_group1").rename({"housetype_905L": "person_housetype"})

    # Here we have num_goup1 and num_group2, so we need to aggregate again.
    train_credit_bureau_b_2_feats = train_credit_bureau_b_2.group_by("case_id").agg(
        pl.col("pmts_pmtsoverdue_635A").max().alias("pmts_pmtsoverdue_635A_max"),
        (pl.col("pmts_dpdvalue_108P") > 31).max().alias("pmts_dpdvalue_108P_over31")
    )

    # We will process in this examples only A-type and M-type columns, so we need to select them.
    selected_static_cols = []
    for col in train_static.columns:
        if col[-1] in ("A", "M"):
            selected_static_cols.append(col)
    # print(selected_static_cols)

    selected_static_cb_cols = []
    for col in train_static_cb.columns:
        if col[-1] in ("A", "M"):
            selected_static_cb_cols.append(col)
    # print(selected_static_cb_cols)

    # Join all tables together.
    data = train_basetable.join(
        train_static.select(["case_id"]+selected_static_cols), how="left", on="case_id"
    ).join(
        train_static_cb.select(["case_id"]+selected_static_cb_cols), how="left", on="case_id"
    ).join(
        train_person_1_feats_1, how="left", on="case_id"
    ).join(
        train_person_1_feats_2, how="left", on="case_id"
    ).join(
        train_credit_bureau_b_2_feats, how="left", on="case_id"
    )
    # print(data)

    ## test Feature Engineering
    test_person_1_feats_1 = test_person_1.group_by("case_id").agg(
    pl.col("mainoccupationinc_384A").max().alias("mainoccupationinc_384A_max"),
    (pl.col("incometype_1044T") == "SELFEMPLOYED").max().alias("mainoccupationinc_384A_any_selfemployed")
    )
    # print(test_person_1_feats_1)

    test_person_1_feats_2 = test_person_1.select(["case_id", "num_group1", "housetype_905L"]).filter(
        pl.col("num_group1") == 0
    ).drop("num_group1").rename({"housetype_905L": "person_housetype"})
    # print(test_person_1_feats_2)

    test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by("case_id").agg(
        pl.col("pmts_pmtsoverdue_635A").max().alias("pmts_pmtsoverdue_635A_max"),
        (pl.col("pmts_dpdvalue_108P") > 31).max().alias("pmts_dpdvalue_108P_over31")
    )
    # print(test_credit_bureau_b_2_feats)

    data_submission = test_basetable.join(
        test_static.select(["case_id"]+selected_static_cols), how="left", on="case_id"
    ).join(
        test_static_cb.select(["case_id"]+selected_static_cb_cols), how="left", on="case_id"
    ).join(
        test_person_1_feats_1, how="left", on="case_id"
    ).join(
        test_person_1_feats_2, how="left", on="case_id"
    ).join(
        test_credit_bureau_b_2_feats, how="left", on="case_id"
    )
    # print(data_submission)

    case_ids = data["case_id"].unique().shuffle(seed=1)
    case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)
    case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)
    # print(case_ids_train.len(), case_ids_valid.len(), case_ids_test.len())

    cols_pred = []
    for col in data.columns:
        if col[-1].isupper() and col[:-1].islower():
            cols_pred.append(col)
    # print(cols_pred)

    base_train, X_train, y_train = from_polars_to_pandas(data, cols_pred, case_ids_train)
    base_valid, X_valid, y_valid = from_polars_to_pandas(data, cols_pred, case_ids_valid)
    base_test, X_test, y_test = from_polars_to_pandas(data, cols_pred, case_ids_test)
    # print(X_train.shape, X_valid.shape, X_test.shape)

    for df in [X_train, X_valid, X_test]:
        df = convert_strings(df)
    # print(f"Train: {X_train.shape}")
    # print(f"Valid: {X_valid.shape}")
    # print(f"Test: {X_test.shape}")

    ## Train model
    lgb_train = lgb.Dataset(X_train, label=y_train)
    lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)
    # print(lgb_train, lgb_valid)

    params = {
        "boosting_type": "gbdt",
        "objective": "binary",
        "metric": "auc",
        "max_depth": 3,
        "num_leaves": 31,
        "learning_rate": 0.05,
        "feature_fraction": 0.9,
        "bagging_fraction": 0.8,
        "bagging_freq": 5,
        "n_estimators": 1000,
        "verbose": -1,
    }

    gbm = lgb.train(
        params,
        lgb_train,
        valid_sets=lgb_valid,
        callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]
    )

    # for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:
    #     y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)
    #     base["score"] = y_pred

    # print(f'The AUC score on the train set is: {roc_auc_score(base_train["target"], base_train["score"])}') 
    # print(f'The AUC score on the valid set is: {roc_auc_score(base_valid["target"], base_valid["score"])}') 
    # print(f'The AUC score on the test set is: {roc_auc_score(base_test["target"], base_test["score"])}')  

    # stability_score_train = gini_stability(base_train)
    # stability_score_valid = gini_stability(base_valid)
    # stability_score_test = gini_stability(base_test)

    # print(f'The stability score on the train set is: {stability_score_train}') 
    # print(f'The stability score on the valid set is: {stability_score_valid}') 
    # print(f'The stability score on the test set is: {stability_score_test}')  

    # X_submission = data_submission[cols_pred].to_pandas()
    # X_submission = convert_strings(X_submission)
    # categorical_cols = X_train.select_dtypes(include=['category']).columns

    # for col in categorical_cols:
    #     train_categories = set(X_train[col].cat.categories)
    #     submission_categories = set(X_submission[col].cat.categories)
    #     new_categories = submission_categories - train_categories
    #     X_submission.loc[X_submission[col].isin(new_categories), col] = "Unknown"
    #     new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)
    #     X_train[col] = X_train[col].astype(new_dtype)
    #     X_submission[col] = X_submission[col].astype(new_dtype)

    # y_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)

    # submission = pd.DataFrame({
    #     "case_id": data_submission["case_id"].to_numpy(),
    #     "score": y_submission_pred
    # }).set_index('case_id')
    # submission.to_csv("./submission.csv")